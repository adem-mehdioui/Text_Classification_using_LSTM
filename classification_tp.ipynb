{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with a dataset that contains movie reviews labeled as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "Index(['review', 'sentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"imdb-dataset/IMDB Dataset.csv\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "#Affichage des colonnes du dataset\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ADEM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADEM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    one reviewers mentioned watching 1 oz episode ...\n",
      "1    wonderful little production br br filming tech...\n",
      "2    thought wonderful way spend time hot summer we...\n",
      "3    basically 's family little boy jake thinks 's ...\n",
      "4    petter mattei 's `` love time money '' visuall...\n",
      "Name: cleaned_reviews, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Effectuez des opérations de nettoyage des textes (suppression de la ponctuation, des stop words, mise en minuscules, etc.)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['cleaned_reviews'] = df['review'].apply(lambda x: ' '.join([word for word in word_tokenize(x.lower()) if word not in stop_words and word not in string.punctuation]))\n",
    "\n",
    "print(df['cleaned_reviews'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodage des labels pour transformer les labels catégoriels en valeurs numériques\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['sentiment_encoded'] = le.fit_transform(df['sentiment'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Divisez les données en ensembles d'entraînement et de test.\n",
    "#Using train_test_split to divide  data into training and test sets.\n",
    "\n",
    "#feature_column is  review column      and    label_column is  sentiment  \n",
    "\n",
    "\n",
    "# Features and labels\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Encoder les labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f'Training set size: {X_train.shape[0]}')\n",
    "print(f'Test set size: {X_test.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de X_train_pad: (40000, 100)\n",
      "Taille de X_test_pad: (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "# Utilisation de pad_sequences de Keras pour que toutes les séquences aient la même longueur.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "#vérification de la taille des séquences\n",
    "\n",
    "print(\"Taille de X_train_pad:\", X_train_pad.shape)\n",
    "print(\"Taille de X_test_pad:\", X_test_pad.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Early Stopping : Utilisez EarlyStopping de Keras pour surveiller la perte de validation (val_loss) et arrêter l'entraînement lorsque celle-ci cesse de s'améliorer.\n",
    "\n",
    "2.\tLearning Rate Decay : Proposez une fonction de décroissance pour le taux d'apprentissage, par exemple en utilisant ReduceLROnPlateau de Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADEM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 44ms/step - accuracy: 0.7752 - loss: 0.4538 - val_accuracy: 0.8621 - val_loss: 0.3126\n",
      "Epoch 2/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 44ms/step - accuracy: 0.8831 - loss: 0.2823 - val_accuracy: 0.8692 - val_loss: 0.3077\n",
      "Epoch 3/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 44ms/step - accuracy: 0.9080 - loss: 0.2292 - val_accuracy: 0.8672 - val_loss: 0.3523\n",
      "Epoch 4/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 44ms/step - accuracy: 0.9236 - loss: 0.1904 - val_accuracy: 0.8704 - val_loss: 0.3282\n",
      "Epoch 5/20\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 45ms/step - accuracy: 0.9422 - loss: 0.1529 - val_accuracy: 0.8673 - val_loss: 0.3365\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.8657 - loss: 0.3071\n",
      "Loss: 0.3077445924282074\n",
      "Accuracy: 0.8691999912261963\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Définir le callback EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # métrique à surveiller\n",
    "    patience=3,          # nombre d'époques à attendre après la dernière amélioration\n",
    "    restore_best_weights=True  # restaurer les poids du meilleur modèle\n",
    ")\n",
    "\n",
    "\n",
    "# Définir le callback ReduceLROnPlateau   de  décroissance de taux d'apprentissage  (Learning Rate Decay)  \n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # métrique à surveiller\n",
    "    factor=0.1,           # facteur de réduction du taux d'apprentissage (nouveau_lr = lr * factor)\n",
    "    patience=5,           # nombre d'époques à attendre après la dernière amélioration avant de réduire le taux d'apprentissage\n",
    "    min_lr=0.0001,        # taux d'apprentissage minimal\n",
    "    verbose=1             # afficher des messages pour indiquer les réductions de taux d'apprentissage\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# utiliser les 2 callback lors de l'entraînement de modèle plus tard\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
